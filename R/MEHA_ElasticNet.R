#' Solving Elastic Net Problem Based on MEHA
#' @description Elastic Net combines the penalties of Lasso (L1 regularization)
#'     and Ridge (L2 regularization) methods. Thus the model introduces two
#'     hyperparameters: \code{x1}, which controls the strength of L1 regularization;
#'     and \code{x2}, which controls the strength of L2 regularization.
#'     The purpose of this function is to determine the optimal feature
#'     coefficients \code{y} and the hyperparameters \code{x1} and \code{x2} of the
#'     elastic net based on the input training and validation sets using MEHA.
#' @references Liu, R., Liu, Z., Yao, W., Zeng, S., & Zhang, J. (2024).
#'     "Rethinking Moreau Envelope for Nonconvex Bi-Level Optimization: A Single-loop and Hessian-free Solution Strategy."
#'     Available at: https://openreview.net/forum?id=i6EtCiIK4a
#'
#'     Gao, L., Ye, J. J., Yin, H., Zeng, S., & Zhang, J. (2022).
#'     "Value function based difference-of-convex algorithm for bilevel hyperparameter selection problems."
#'     Available at: https://proceedings.mlr.press/v162/gao22j.html
#' @param A_val Input feature matrix of the validation set, with dimensions n by p,
#'     where n is the total number of validation samples and p is the number of
#'     features. Each row represents an observation vector.
#' @param b_val Quantitative response variable of validation set.
#' @param A_tr Input feature matrix of training set, with dimension n' by p,
#'     where n' is the total number of training samples and p is feature number.
#' @param b_tr Quantitative response variable of training set.
#' @param N Total iterations of MEHA. Default is 500.
#' @param alpha Projection stepsize of \code{x}, which is fixed. Default is 0.001.
#'     Specifically, you can edit the code and utilize the inverse power learning rate annealing strategy to dynamically adjust it.
#' @param beta  Proximal gradient stepsize of \code{y} which is fixed. Default is 1e-5.
#'     Specifically, you can edit the code and utilize the inverse power learning rate annealing strategy to dynamically adjust it.
#' @param eta Proximal gradient stepsize of \code{theta}, which is fixed.
#'     Default is 1e-5.
#' @param gamma Moreau envelope parameter. Default is 2.
#' @param c \eqn{\underline{c}} in MEHA, which is used to generate the penalty parameter \eqn{c_k} in LV-HBA by \eqn{c_k = \underline{c}(k+1)^p}.
#'     Default is 1.
#' @param p Default is 0.48.
#' @param auto_tuning When alpha, beta, eta are fixed, whether an auto-hyperparameter-tuning is needed.
#'     Default is \code{FALSE}.
#' @param temperature Temperature of simulating annealing method for auto-
#'     hyperparameter-tuning. Default is 0.1.
#'
#' @return
#'
#'   \item{x}{The first value is \code{x1} (Lasso penalty strength), while the
#'    second value is \code{x2} (Ridge penalty strength).}
#'   \item{y}{The feature coefficient vector, of dimension p, where p is the
#'       feature number.}
#'   \item{theta}{The moreau envelope parameter in MEHA, which is of the same scale as variable y}
#'   \item{Xconv}{Describe the relative convergence situation of sequence x generated by MEHA,
#'       which is computed by \eqn{||x^{k+1}-x^k|| / ||x^K||} based on l2-norm.}
#'   \item{Yconv}{Describe the relative convergence situation of sequence y generated by MEHA,
#'       which is computed by \eqn{||y^{k+1}-y^k|| / ||y^K||} based on l2-norm.}
#'   \item{Thetaconv}{Describe the relative convergence situation of sequence theta generated by MEHA,
#'       which is computed by \eqn{||theta^{k+1}-theta^k|| / ||theta^K||} based on l2-norm.}
#'   \item{Fseq}{The upper function value sequence generated by the iteration based on validation set.}
#'
#'
#' @export
#'

MEHA_Elasticnet = function(A_val, b_val, A_tr, b_tr, N = 500, alpha = 1e-3, beta = 1e-5, eta = 1e-5, gamma = 2, c = 2, p = 0.48, auto_tuning = FALSE, temperature = 0.1){

  library(progress)
  library(truncnorm)


  main_fun <- function(A_val, b_val, A_tr, b_tr, N, alpha, beta, eta, gamma = gamma, c = c, p = p){

    p_fea = dim(A_val)[2]
    #print(p_fea)

    x = matrix(rep(1), nrow = 2)
    y = matrix(rep(10), nrow = p_fea)
    theta = matrix(rep(15), nrow = p_fea)
    e2 = matrix(rep(0), nrow = 2)
    ep = matrix(rep(0), nrow = p_fea)

    # objective function
    F = function(x,y){
      result = 0.5*norm(A_val %*% y - b_val, type = "2")^2
      return(result)
    }

    phi = function(x, y){
      result = 0.5*norm(A_tr %*%  y - b_tr, type = "2")^2 + cbind(norm(y, type = "1"), 0.5*norm(y,type = "2")^2 ) %*% x
      return(result)
    }


    # update function
    F_x = function(x, y){
      result = t(matrix(rep(0, 2), ncol = 2))
      return(result)
    }

    F_y = function(x, y){
      result = t(t(A_val %*% y - b_val) %*% A_val)
      return(result)
    }

    f_x = function(x, y){
      result = rbind(0, 0.5*norm(y,type = "2")^2 )
      return(result)
    }

    f_y = function(x, y){
      result = t(t(A_tr %*% y - b_tr) %*% A_tr + x[2] * t(y))
      return(result)
    }

    g_x = function(x, y){
      result =  rbind(norm(y,type = "1"), 0)
      return(result)
    }

    ## proximal operator
    prox_eta = function(x, y, theta){
      z = theta - eta * (f_y(x, theta) + (theta - y) / gamma)
      lambda = eta * matrix(rep(x[1], p_fea), nrow = p_fea)
      result = sign(z) * pmax((abs(z) - lambda), 0*ep)
    }

    prox_beta = function(x, y, dky){
      z = y - beta * dky
      lambda = eta * matrix(rep(x[1], p_fea), nrow = p_fea)
      result = sign(z) * pmax((abs(z) - beta * lambda), 0*ep)
    }



    ## algorithm
    arrF = numeric(N) # F(x^{k+1},y^{k+1})
    res1 = numeric(N) #||x^{k+1}-x^k|| / ||x^K||
    res2 = numeric(N) #||y^{k+1}-y^k|| / ||y^K||
    res3 = numeric(N) #||theta^{k+1}-theta^k|| / ||theta^K||

    for (k in 1:N) {
      xk = x
      yk = y
      thetak = theta
      # ck = 0.49
      ck = c*(k + 1)^p
      theta = prox_eta(x, y, theta)
      dkx = (1/ck) * F_x(x, y) + f_x(x, y) + g_x(x, y) - f_x(x, theta) - g_x(x, theta)
      x = pmax(x - alpha * dkx,0*e2)
      dky = (1/ck) * F_y(x, y) + f_y(x, y) - (y - theta)/gamma
      y = prox_beta(x, y, dky)
      res1[k] = norm(x - xk , "2") / norm(xk, "2")
      res2[k] = norm(y - yk, "2") / norm(yk, "2")
      res3[k] = norm(theta - thetak, "2") / norm(thetak, "2")
      arrF[k] = F(x, y)
    }

    return(list(x = x, y = y, theta = theta, Xconv = res1, Yconv = res2, Thetaconv= res3, Fseq = arrF))
  }

  if(auto_tuning == TRUE){
    message("\n","Auto-hyperparameters-tuning is proceeding now.")

    iter <- 100
    T <- temperature

    pb <- progress_bar$new(
      total = iter,
      format = "  Finished :current/:total [:bar] :percent  remaining time :eta"
    )


    alpha.seq <- numeric(iter)
    beta.seq <- numeric(iter)
    eta.seq <- numeric(iter)
    value <- numeric(iter)

    alpha.seq[1] <- alpha
    beta.seq[1] <- beta
    eta.seq[1] <- eta

    result = main_fun(A_val, b_val, A_tr, b_tr, N, alpha = alpha.seq[1], beta = beta.seq[1], eta = eta.seq[1], gamma = gamma, c = c, p = p)
    value[1] <- result$Fseq[order(result$Fseq, decreasing = FALSE)[1]]



    set.seed(123)
    for (j in 2:iter) {
      T <- exp(-0.0001*j)
      alpha.seq[j] <- rtruncnorm(n = 1, a = 0, mean = alpha.seq[j-1], sd = 1e-3)
      beta.seq[j] <- rtruncnorm(n = 1, a = 0, mean = beta.seq[j-1], sd = 1e-6)
      eta.seq[j] <- rtruncnorm(n = 1, a = 0, mean = eta.seq[j-1], sd = 1e-6)
      result = main_fun(A_val, b_val, A_tr, b_tr, N, alpha = alpha.seq[j], beta = beta.seq[j], eta = eta.seq[j], gamma = gamma, c = c, p = p)
      candidate <- result$Fseq[order(result$Fseq, decreasing = FALSE)[1]]
      if(candidate > value[j-1] & runif(n = 1) > exp((value[j-1]-candidate)/T)){
        value[j] <- value[j-1]
      } else {
        value[j] <- candidate
      }
      pb$tick()
    }

    opt_index <- order(value)[1]

    #plot(value, type = "l", xlab = "iteration")

    cat("\n", "Auto-hyperparameters-tuning is done.")
    cat("\nFinal hyper-paramaters (alpha,beta,eta) are chosen as:",c(alpha.seq[opt_index], beta.seq[opt_index], eta.seq[opt_index]))

    return(main_fun(A_val, b_val, A_tr, b_tr, N, alpha = alpha.seq[opt_index], beta = beta.seq[opt_index], eta = eta.seq[opt_index], gamma, c, p))

  } else{
    return(main_fun(A_val, b_val, A_tr, b_tr, N, alpha, beta, eta, gamma, c, p))
  }


}
