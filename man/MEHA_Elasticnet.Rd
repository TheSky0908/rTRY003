% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/MEHA_ElasticNet.R
\name{MEHA_Elasticnet}
\alias{MEHA_Elasticnet}
\title{Solving Elastic Net Problem Based on MEHA}
\usage{
MEHA_Elasticnet(
  A_val,
  b_val,
  A_tr,
  b_tr,
  N = 500,
  alpha = 0.001,
  beta = 1e-05,
  eta = 1e-05,
  gamma = 2,
  c = 2,
  p = 0.48,
  auto_tuning = FALSE,
  temperature = 0.1
)
}
\arguments{
\item{A_val}{Input feature matrix of the validation set, with dimensions n by p,
where n is the total number of validation samples and p is the number of
features. Each row represents an observation vector.}

\item{b_val}{Quantitative response variable of validation set.}

\item{A_tr}{Input feature matrix of training set, with dimension n' by p,
where n' is the total number of training samples and p is feature number.}

\item{b_tr}{Quantitative response variable of training set.}

\item{N}{Total iterations of MEHA. Default is 500.}

\item{alpha}{Projection stepsize of \code{x}, which is fixed. Default is 0.001.
Specifically, you can edit the code and utilize the inverse power learning rate annealing strategy to dynamically adjust it.}

\item{beta}{Proximal gradient stepsize of \code{y} which is fixed. Default is 1e-5.
Specifically, you can edit the code and utilize the inverse power learning rate annealing strategy to dynamically adjust it.}

\item{eta}{Proximal gradient stepsize of \code{theta}, which is fixed.
Default is 1e-5.}

\item{gamma}{Moreau envelope parameter. Default is 2.}

\item{c}{\eqn{\underline{c}} in MEHA, which is used to generate the penalty parameter \eqn{c_k} in LV-HBA by \eqn{c_k = \underline{c}(k+1)^p}.
Default is 1.}

\item{p}{Default is 0.48.}

\item{auto_tuning}{When alpha, beta, eta are fixed, whether an auto-hyperparameter-tuning is needed.
Default is \code{FALSE}.}

\item{temperature}{Temperature of simulating annealing method for auto-
hyperparameter-tuning. Default is 0.1.}
}
\value{
\item{x}{The first value is \code{x1} (Lasso penalty strength), while the
second value is \code{x2} (Ridge penalty strength).}
\item{y}{The feature coefficient vector, of dimension p, where p is the
feature number.}
\item{theta}{The moreau envelope parameter in MEHA, which is of the same scale as variable y}
\item{Xconv}{Describe the relative convergence situation of sequence x generated by MEHA,
which is computed by \eqn{||x^{k+1}-x^k|| / ||x^K||} based on l2-norm.}
\item{Yconv}{Describe the relative convergence situation of sequence y generated by MEHA,
which is computed by \eqn{||y^{k+1}-y^k|| / ||y^K||} based on l2-norm.}
\item{Thetaconv}{Describe the relative convergence situation of sequence theta generated by MEHA,
which is computed by \eqn{||theta^{k+1}-theta^k|| / ||theta^K||} based on l2-norm.}
\item{Fseq}{The upper function value sequence generated by the iteration based on validation set.}
}
\description{
Elastic Net combines the penalties of Lasso (L1 regularization)
and Ridge (L2 regularization) methods. Thus the model introduces two
hyperparameters: \code{x1}, which controls the strength of L1 regularization;
and \code{x2}, which controls the strength of L2 regularization.
The purpose of this function is to determine the optimal feature
coefficients \code{y} and the hyperparameters \code{x1} and \code{x2} of the
elastic net based on the input training and validation sets using MEHA.
}
\references{
Liu, R., Liu, Z., Yao, W., Zeng, S., & Zhang, J. (2024).
"Rethinking Moreau Envelope for Nonconvex Bi-Level Optimization: A Single-loop and Hessian-free Solution Strategy."
Available at: https://openreview.net/forum?id=i6EtCiIK4a

\if{html}{\out{<div class="sourceCode">}}\preformatted{Gao, L., Ye, J. J., Yin, H., Zeng, S., & Zhang, J. (2022).
"Value function based difference-of-convex algorithm for bilevel hyperparameter selection problems."
Available at: https://proceedings.mlr.press/v162/gao22j.html
}\if{html}{\out{</div>}}
}
