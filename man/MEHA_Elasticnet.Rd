% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/MEHA_ElasticNet.R
\name{MEHA_Elasticnet}
\alias{MEHA_Elasticnet}
\title{Solving Elastic Net Problem Based on MEHA}
\usage{
MEHA_Elasticnet(
  A_val,
  b_val,
  A_tr,
  b_tr,
  N = 500,
  alpha = 0.001,
  beta = 1e-05,
  eta = 1e-05,
  gamma = 2,
  c = 2,
  p = 0.48,
  auto_tuning = FALSE,
  temperature = 0.1
)
}
\arguments{
\item{A_val}{Input feature matrix of the validation set, with dimensions n by p,
where n is the total number of validation samples and p is the number of
features. Each row represents an observation vector.}

\item{b_val}{Quantitative response variable of validation set.}

\item{A_tr}{Input feature matrix of training set, of dimension n' by p,
where n' is the total number of training samples and p is feature number.}

\item{b_tr}{Quantitative response variable of training set.}

\item{N}{Total iterations. Default is 500.}

\item{alpha}{Proximal gradient stepsize of \code{x}. Default is 0.001.}

\item{beta}{Proximal gradient stepsize of \code{y}. Default is 1e-5.}

\item{eta}{Proximal gradient stepsize of the proxima \code{theta}.
Default is 1e-5.}

\item{gamma}{Moreau envelope parameter. Default is 2.}

\item{c}{Penalty strength of the Moreau envelope inequality constraint.
Default is 2.}

\item{p}{Default is 0.48.}

\item{auto_tuning}{Whether an auto-hyperparameter-tuning is needed.
Default is \code{FALSE}.}

\item{temperature}{Temperature of simulating annealing method for auto-
hyperparameter-tuning. Default is 0.1.}
}
\value{
\item{x}{The first value is \code{x1} (Lasso penalty strength), while the
second value is \code{x2} (Ridge penalty strength).}
\item{y}{The feature coefficient vector, of dimension p, where p is the
feature number.}
\item{theta}{to}
\item{Xconv}{Describe the relative convergence situation of sequence X,
based on l2-norm.}
\item{Yconv}{Describe the relative convergence situation of sequence Y,
based on l2-norm.}
\item{Thetaconv}{Describe the relative convergence situation of sequence theta,
based on l2-norm.}
\item{Fseq}{The upper function value in each iteration.}
}
\description{
Elastic Net combines the penalties of Lasso (L1 regularization)
and Ridge (L2 regularization) methods. Thus the model introduces two
hyperparameters: \code{x1}, which controls the strength of L1 regularization;
and \code{x2}, which controls the strength of L2 regularization.
The purpose of this function is to determine the optimal feature
coefficients \code{y} and the hyperparameters \code{x1} and \code{x2} of the
elastic net based on the input training and validation sets using MEHA.
}
