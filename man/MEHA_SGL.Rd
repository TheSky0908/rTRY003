% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/MEHA_SGL.R
\name{MEHA_SGL}
\alias{MEHA_SGL}
\title{Sparse Group Lasso Based on MEHA}
\usage{
MEHA_SGL(
  A_val,
  b_val,
  A_tr,
  b_tr,
  group,
  N = 300,
  alpha = 1e-05,
  beta = 1e-05,
  eta = 1e-05,
  gamma = 1,
  c = 1,
  p = 0.3,
  auto_tuning = FALSE,
  temperature = 0.01
)
}
\arguments{
\item{A_val}{Input feature matrix of validation set, each row of which
is an observation vector.}

\item{b_val}{Quantitative response variable of validation set.}

\item{A_tr}{Input feature matrix of training set.}

\item{b_tr}{Quantitative response variable of training set.}

\item{group}{A vector of length M (where M denotes the total group number) to
describe the feature group information, with each element representing the
specific number of features in each group.}

\item{N}{Total iterations. Default is 300.}

\item{alpha}{Proximal gradient stepsize of \code{x}. Default is 1e-5.}

\item{beta}{Proximal gradient stepsize of \code{y}. Default is 1e-5.}

\item{eta}{Proximal gradient stepsize of the proxima \code{theta}.
Default is 1e-5.}

\item{gamma}{Moreau envelope parameter. Default is 1.}

\item{c}{Penalty strength of the Moreau envelope inequality constraint.
Default is 1.}

\item{p}{Default is 0.3.}

\item{auto_tuning}{Whether an auto-hyperparameter-tuning is needed.
Default is \code{FALSE}.}

\item{temperature}{Temperature of simulating annealing method for auto-
hyperparameter-tuning. Default is 0.01.}
}
\value{
\item{x}{A vector of length (M+1), where M denotes the total group number.
The first M values are the within-group penalty strengths, while the last
value represents the group penalty.}
\item{y}{The feature coefficient vector, of dimension p, where p is the
feature number.}
\item{theta}{to}
\item{Xconv}{Describe the relative convergence situation of sequence X,
based on l2-norm.}
\item{Yconv}{Describe the relative convergence situation of sequence Y,
based on l2-norm.}
\item{Thetaconv}{Describe the relative convergence situation of sequence theta,
based on l2-norm.}
\item{Fseq}{The upper function value in each iteration.}
}
\description{
Sparse Group Lasso extends the Lasso and Group Lasso techniques
by enforcing sparsity both at the group level and within each group. Sparse
Group Lasso applies an L2 penalty to groups of coefficients and an L1
penalty to individual coefficients, encouraging sparsity within and between
groups. This approach introduces two hyperparameters: within-group penalty
and group penalty which control the extent of regularization.
The purpose of this function is to determine the optimal feature
coefficients \code{y} and the hyperparameters \code{x} of the sparse group
Lasso based on the input training and validation sets using MEHA.
}
\examples{
library(mvtnorm)

N <- 100 #sample size
p <- 600 #feature number
M <- 30 #feature group number
beta_i <- c(1:5,rep(0,195))
true_beta <- rep(beta_i,3) #true coefficient
set.seed(123)
a <- matrix(rnorm(3*N*p), ncol = p)
epsilon <- matrix(rnorm(3*N), ncol = 1) #residual
sigma <- norm(a \%*\% true_beta, type = "2") / norm(epsilon, type = "2")
b <- a \%*\% true_beta + sigma/2 * epsilon
SNR <- norm(a \%*\% true_beta, type = "2") / norm(b - a \%*\% true_beta,
 type = "2")
cat("SNR is", SNR) #the signal-to-noise ratio is controlled at 2

set.seed(123)
#split into training, validation and testing set
A_val = a[1:100, ]
b_val = b[1:100]
A_tr = a[101:200, ]
b_tr = b[101:200]
A_test = a[201:300, ]
b_test = b[201:300]

group <- as.matrix(rep(x = p/M, M)) #feature group information

result = MEHA_SGL(A_val, b_val, A_tr, b_tr, group,N = 200,
    auto_tuning = TRUE, alpha = 3e-4)

plot(result$Yconv,ylab = expression(paste("||",y^k - y^(k - 1),"||/||",
y^(k - 1),"||")), xlab = "iteration",type = "l",
 main = "Y Sequence Convergence")

plot(result$Fseq, xlab = "iteration", type = "l",
ylab = expression(F(x^k,y^k)))

norm(A_val \%*\% result$y - b_val, type = "2")/length(b_val)


}
